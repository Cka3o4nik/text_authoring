{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10bbe6360f9659217f9f40455301aa72d6b614a6"
   },
   "outputs": [],
   "source": [
    "!pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10bbe6360f9659217f9f40455301aa72d6b614a6"
   },
   "outputs": [],
   "source": [
    "!pip install -U dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10bbe6360f9659217f9f40455301aa72d6b614a6"
   },
   "outputs": [],
   "source": [
    "!pip install -U nltk #==3.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4638a871a58694a77473a590728ebf25047cd6eb"
   },
   "source": [
    "## Lets get some real data and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = r\"input/Десперо/DiCamillo Kate. The Tale of Despereaux - royallib.com.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url = r\"https://uc1ba2935594911273cb019a979f.dl.dropboxusercontent.com/cd/0/get/AwfcsVbUXmXzUdQqZ-yXILh1SMOtwh969xh-RwWKfPvVaHRsbKBC749OabQS8cn9SkSIMKA8EaV3PQLin_HgQzlzfjIlRaDiKG3rWpP3FQQfVg/file?_download_id=561601465769664458798338779964772424300021697661964057822012315&_notify_domain=www.dropbox.com&dl=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with request.urlopen(source_url) as f:\n",
    "    source_text_dec = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(source_path, 'rt') as f:\n",
    "    source_text_dec = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182767"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_text_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Tale of Despereaux\\r\\n\\r\\nby Kate DiCamillo\\r\\n\\r\\nFor Luke, who asked for\\r\\n\\r\\nthe story of an unlikely h'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_text_dec[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized = sent_tokenize(source_text_dec)\n",
    "len(sent_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Tale of Despereaux\\r\\n\\r\\nby Kate DiCamillo\\r\\n\\r\\nFor Luke, who asked for\\r\\n\\r\\nthe story of an unlikely hero\\r\\n\\r\\n\\r\\n\\r\\nThe world is dark, and light is precious.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42399"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = word_tokenize(source_text_dec)\n",
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "9daa634f65b63882f7192a9489fba198970f3771",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'tale',\n",
       " 'of',\n",
       " 'despereaux',\n",
       " 'by',\n",
       " 'kate',\n",
       " 'dicamillo',\n",
       " 'for',\n",
       " 'luke',\n",
       " ',',\n",
       " 'who',\n",
       " 'asked',\n",
       " 'for',\n",
       " 'the',\n",
       " 'story',\n",
       " 'of',\n",
       " 'an',\n",
       " 'unlikely',\n",
       " 'hero',\n",
       " 'the',\n",
       " 'world',\n",
       " 'is',\n",
       " 'dark',\n",
       " ',',\n",
       " 'and',\n",
       " 'light',\n",
       " 'is',\n",
       " 'precious',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tale'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "91c93337cfea53b0c15a6e8617395dab872fb140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tale of Despereaux\r\n",
      "\r\n",
      "by Kate DiCamillo\r\n",
      "\r\n",
      "For Luke, who asked for\r\n",
      "\r\n",
      "the story of an unlikely h\n"
     ]
    }
   ],
   "source": [
    "print(source_text_dec[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "3eef00b8b1a95043e1c8bdd6912ae22060198e5c"
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af4102fcbf643e09319ca6394c175ca64082fd19"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "sent_tokenized = [list(map(str.lower, tokenized)) \n",
    "                  for sent in sent_tokenize(source_text_dec)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3eef00b8b1a95043e1c8bdd6912ae22060198e5c"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, sent_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32c5ad89e3e61b143e940d50a0d0ee602dadfe3f"
   },
   "source": [
    "# Training an N-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff1ff506e3a14df8283cb93b0a45f86862d3e3c7"
   },
   "source": [
    "Having prepared our data we are ready to start training a model. As a simple example, let us train a Maximum Likelihood Estimator (MLE).\n",
    "\n",
    "We only need to specify the highest ngram order to instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2635458b0f3fed618bfae6a0705ccb11555bfb0b"
   },
   "outputs": [],
   "source": [
    "model = MLE(n) # Lets train a 3-grams model, previously we set n=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69cdd0e5babaf059a7b9bc87dc0a6e261cef6cb2"
   },
   "source": [
    "Initializing the MLE model, creates an empty vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93b95f89f020585dabfbcd6c41273c823a91e882"
   },
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb567e332a0089c86e388a8c93f32d1a31737a29"
   },
   "source": [
    "... which gets filled as we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a31ecc7f6df30a42df9ed6a79039b50f734299cf"
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fd7e68c2cdacbe1901fd3b8c05c440706d19100"
   },
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74622c58a2df2ac08c8b0c79644d1f87f4014f44"
   },
   "source": [
    "The vocabulary helps us handle words that have not occurred during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_uuid": "e5670e34f9d1950f6a1515d57db18092d39484f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "print(model.vocab.lookup(tokenized_text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "cfec70768e8976073dda05f702f3f210a33dc1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('despereaux', 'by', 'kate', 'is', '<UNK>', '<UNK>', '<UNK>', '.')\n"
     ]
    }
   ],
   "source": [
    "# If we lookup the vocab on unseen sentences not from the training data, \n",
    "# it automatically replace words not in the vocabulary with `<UNK>`.\n",
    "print(model.vocab.lookup('despereaux by kate is never random lah .'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83a71e4b25b53795dedce91c1a5686b7e22279e0"
   },
   "source": [
    "Moreover, in some cases we want to ignore words that we did see during training but that didn't occur frequently enough, to provide us useful information. \n",
    "\n",
    "You can tell the vocabulary to ignore such words using the `unk_cutoff` argument for the vocabulary lookup, To find out how that works, check out the docs for the [`nltk.lm.vocabulary.Vocabulary` class](https://github.com/nltk/nltk/blob/develop/nltk/lm/vocabulary.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0310a2f3c7d9e90f4574b44838336ca1e4ef2ef"
   },
   "source": [
    "**Note:** For more sophisticated ngram models, take a look at [these objects from `nltk.lm.models`](https://github.com/nltk/nltk/blob/develop/nltk/lm/models.py):\n",
    "\n",
    " - `Lidstone`: Provides Lidstone-smoothed scores.\n",
    " - `Laplace`: Implements Laplace (add one) smoothing.\n",
    " - `InterpolatedLanguageModel`: Logic common to all interpolated language models (Chen & Goodman 1995).\n",
    " - `WittenBellInterpolated`: Interpolated version of Witten-Bell smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af5a0851a2f8707ea2e172681342ed3ecd872328"
   },
   "source": [
    "# N-gram Language Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', '<s>')\n",
      "('<s>', 'the')\n",
      "('the', 'tale')\n",
      "('tale', 'of')\n",
      "('of', 'despereaux')\n",
      "('despereaux', 'by')\n",
      "('by', 'kate')\n",
      "('kate', 'dicamillo')\n",
      "('dicamillo', 'for')\n",
      "('for', 'luke')\n",
      "('luke', ',')\n",
      "(',', 'who')\n",
      "('who', 'asked')\n",
      "('asked', 'for')\n",
      "('for', 'the')\n",
      "('the', 'story')\n",
      "('story', 'of')\n",
      "('of', 'an')\n",
      "('an', 'unlikely')\n",
      "('unlikely', 'hero')\n",
      "('hero', 'the')\n",
      "('the', 'world')\n",
      "('world', 'is')\n",
      "('is', 'dark')\n",
      "('dark', ',')\n",
      "(',', 'and')\n",
      "('and', 'light')\n",
      "('light', 'is')\n",
      "('is', 'precious')\n",
      "('precious', '.')\n",
      "('.', '</s>')\n"
     ]
    }
   ],
   "source": [
    "for c in model.counts[3]:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "text_trigrams = [ngrams(sent, 3) for sent in tokenized_text]\n",
    "text_bigrams = [ngrams(sent, 2) for sent in tokenized_text]\n",
    "text_unigrams = [ngrams(sent, 1) for sent in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "ngram_counts = NgramCounter(text_trigrams + text_bigrams + text_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d098fd4686bcdc0ab9aa72dbbd4b62a17ee2c332"
   },
   "source": [
    "When it comes to ngram models the training boils down to counting up the ngrams from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfc60d61539269298390044c0d3415bfd28e2b1e"
   },
   "outputs": [],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9b8cac68de80aee9c7cf7b7f2faf2b199ad27cc"
   },
   "source": [
    "This provides a convenient interface to access counts for unigrams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90f4611580c41118747a42cfac4c9d729f02523f"
   },
   "outputs": [],
   "source": [
    "model.counts['despereaux'] # i.e. Count('language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccdf1bf83ba16f0a43f04eb96ca224a968cc81d2"
   },
   "source": [
    "...and bigrams for the phrase \"language is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ba55a83d7c052d1f14923c4046a5f6c86d72c8c"
   },
   "outputs": [],
   "source": [
    "model.counts[['said']]['despereaux'] # i.e. Count('is'|'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88bceda0a01e9f2642dd30a6b341c24a606720b9"
   },
   "source": [
    "... and trigrams for the phrase \"language is never\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "813e30e1fb153002cfb020d430def2510bf43bbc"
   },
   "outputs": [],
   "source": [
    "model.counts[['language', 'despereaux']]['never'] # i.e. Count('never'|'language is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c4338d351cfc3cd785673ad9581ecab161b24c6"
   },
   "source": [
    "And so on. However, the real purpose of training a language model is to have it score how probable words are in certain contexts.\n",
    "\n",
    "This being MLE, the model returns the item's relative frequency as its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "051ef5a06c004a8e8ddb6168ca318bc8c0c9abf4"
   },
   "outputs": [],
   "source": [
    "model.score('despereaux') # P('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab3d48919b8f074342e624a1da441621829a4dbd"
   },
   "outputs": [],
   "source": [
    "model.score('said', 'despereaux'.split())  # P('is'|'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73955908d499f459e19c2cbfaec7d94c513acb91"
   },
   "outputs": [],
   "source": [
    "model.score('never', 'language is'.split())  # P('never'|'language is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "946dde75e8d5b8d8878271359b0d91926abeb199"
   },
   "source": [
    "Items that are not seen during training are mapped to the vocabulary's \"unknown label\" token.  This is \"<UNK>\" by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44038e8cd8d078b734a6b1a803d0795517e2fa82"
   },
   "outputs": [],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2f170a0bbca4b33b5eb1a55a55f0e1b1b18a634"
   },
   "outputs": [],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"leh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9107d1a7c27f30a449e9f370002a6406d5a1396d"
   },
   "outputs": [],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a9ad0be90c72ece6c2a1c00314ea60f41a00f81"
   },
   "source": [
    "To avoid underflow when working with many small score values it makes sense to take their logarithm. \n",
    "\n",
    "For convenience this can be done with the `logscore` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20ef81b7e161df3b9ab27236c33559713b3077ce"
   },
   "outputs": [],
   "source": [
    "model.logscore(\"never\", \"language is\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18f9e0a8d0aba302532b8a84f342d1bf4d3e202a"
   },
   "source": [
    "# Generation using N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f893159fe093aef484c07b37d922de4ac5727834"
   },
   "source": [
    "One cool feature of ngram models is that they can be used to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98989ec4bae592fc98332e759daf9e42bac4213e"
   },
   "outputs": [],
   "source": [
    "print(model.generate(20, random_seed=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b8d07eaf3afae978131573ae127fa0ec3f2e50d"
   },
   "source": [
    "We can do some cleaning to the generated tokens to make it human-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73d0d5e0029e64876100e0f2e368b4835a99efcd"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "949378240cbc8247579b79946b113a3afd039b39"
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9041c514e0458236132fe9b3c42ac2ce651beb92"
   },
   "outputs": [],
   "source": [
    "print(model.generate(28, random_seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "523e7c2373f6a4c4b6a542f1e82369fedb6cbd21"
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 28, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bce2885bd22d0bb1f79c3861f10fde7df6714e40"
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90ddb0e7d0fb52bc77d08425331944c55ee0885e"
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba9546c6800d9e0b0f0af4f0fd31a141af93b12f"
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model \n",
    "\n",
    "The native Python's pickle may not save the lambda functions in the  model, so we can use the `dill` library in place of pickle to save and load the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle \n",
    "\n",
    "with open('kilgariff_ngram_model.pkl', 'wb') as fout:\n",
    "    pickle.dump(model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kilgariff_ngram_model.pkl', 'rb') as fin:\n",
    "    model_loaded = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sent(model_loaded, 20, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4f13ed7359c87e397229104c1bcf18eb20603ad"
   },
   "source": [
    "# Lets try some generating with Donald Trump data!!!\n",
    "\n",
    "\n",
    "**Dataset:** https://www.kaggle.com/kingburrito666/better-donald-trump-tweets#Donald-Tweets!.csv\n",
    "\n",
    "\n",
    "In this part, I'll be munging that data as how I would be doing it at work. \n",
    "I've really no seen the data before but I hope this session would be helpful for you to see how to approach new datasets with the skills you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "caf5fea33f84e06c3cf613136ec33f80e774fc98"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('input/Donald-Tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a15d9d1a754ca357ae79e8390069b08b05320458"
   },
   "outputs": [],
   "source": [
    "trump_corpus = list(df['Tweet_Text'].apply(word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8724b8724cc52b685205047bd1fda649545a6a24"
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5010b607062f6f2dd0ccf011bb9004e1e3dcee7"
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "trump_model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "trump_model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8036f1ae5e0e1438d1d2981026766cdacd4479aa"
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=20, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "defe4e4899cb14300710eb0c7786a6f2e894455d"
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=10, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eba7934efa5a5e6b818e951c5daec9e9f621ca26"
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=50, random_seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ec48d8d44b2f57b16249e12b845f43f52faadca"
   },
   "outputs": [],
   "source": [
    "print(generate_sent(trump_model, num_words=100, random_seed=52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2567d003d69c26233b915df03509b3fd014beed4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
